---
title: "å¤§å‹èªè¨€æ¨¡å‹ Fine-Tuning å®Œæ•´æŒ‡å—ï¼šPEFTã€LoRAã€è³‡æ–™æº–å‚™èˆ‡ç›®éŒ„æ¶æ§‹å¯¦ä½œ"
date: 2025-05-16 12:00:00 +0800
categories: [LLM]
tags: [Fine-Tune, LLM, PEFT, LoRA, Hugging Face, Transformers]
---

# å¤§å‹èªè¨€æ¨¡å‹ Fine-Tuning å®Œæ•´æŒ‡å—ï¼šPEFTã€LoRAã€è³‡æ–™æº–å‚™èˆ‡ç›®éŒ„æ¶æ§‹å¯¦ä½œ

è‡ªå¾ ChatGPT å•ä¸–å¾Œï¼Œå¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLM, Large Language Modelï¼‰çš„ä½¿ç”¨è¶Šä¾†è¶Šæ™®åŠï¼Œä½†çœŸæ­£èƒ½æŒæ¡**å¦‚ä½•é‡å°ç‰¹å®šä»»å‹™é€²è¡Œå¾®èª¿ï¼ˆfine-tuningï¼‰**çš„å·¥ç¨‹å¸«ä»ç„¶ç¨€å°‘ã€‚

> æœ¬æ–‡å°‡å¾è§€å¿µå…¥é–€ã€å¾®èª¿é¡å‹ã€è³‡æ–™æº–å‚™ï¼Œåˆ° Hugging Face + PEFT + LoRA çš„å¯¦ä½œèˆ‡å°ˆæ¡ˆç›®éŒ„æ¶æ§‹ï¼Œå¸¶ä½ å®Œæ•´æŒæ¡ LLM çš„å¾®èª¿èƒ½åŠ›ã€‚

---

## ğŸ§  ç‚ºä½•éœ€è¦å¾®èª¿å¤§å‹èªè¨€æ¨¡å‹ï¼Ÿ

- ğŸ” LLM çš„é€šç”¨èƒ½åŠ›å¼·ï¼Œä½†å°ç‰¹å®šé ˜åŸŸçŸ¥è­˜ä»æœ‰é™ï¼ˆä¾‹å¦‚é‡‘èã€é†«ç™‚ã€ä¼æ¥­å…§éƒ¨è³‡æ–™ï¼‰
- ğŸ¯ Fine-tuning èƒ½è®“æ¨¡å‹å°ˆæ³¨åœ¨ **ç‰¹å®šä»»å‹™** æˆ– **ç‰¹å®šèªèª¿**
- ğŸ’¡ å¾®èª¿å¾Œå¯ç¯€çœæ¨è«–æˆæœ¬ã€æå‡å›æ‡‰ä¸€è‡´æ€§ã€æ”¯æ´ç‰¹å®šæ ¼å¼ç”Ÿæˆ

---

## ğŸ”„ å¾®èª¿é¡å‹æ¦‚è¦½ï¼ˆå¾ç°¡åˆ°é›£ï¼‰

| é¡å‹                | èªªæ˜ |
|---------------------|------|
| Prompt Tuning       | åªè¨“ç·´ prompt prefixï¼Œæ•ˆèƒ½è¼ƒå·®ä½†å¿«é€Ÿè¼•ä¾¿ |
| LoRA / Adapter Tuning | éƒ¨åˆ†åƒæ•¸å¯è¨“ç·´ï¼Œè¼•é‡ã€æ•ˆç‡ä½³ï¼ˆæ¨è–¦ï¼‰ |
| Full Fine-Tuning    | è¨“ç·´æ‰€æœ‰åƒæ•¸ï¼Œæ•ˆæœæœ€ä½³ä½†éœ€é«˜ç®—åŠ›        |

---

## ğŸ§° ä»€éº¼æ˜¯ PEFTï¼Ÿä»€éº¼æ˜¯ LoRAï¼Ÿ

### âœ… PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰

PEFT æ˜¯ Hugging Face æä¾›çš„å·¥å…·å¥—ä»¶ï¼Œæ”¯æ´å„ç¨® **åªèª¿æ•´éƒ¨åˆ†åƒæ•¸çš„å¾®èª¿ç­–ç•¥**ï¼Œå¤§å¹…é™ä½è¨˜æ†¶é«”èˆ‡è¨“ç·´æˆæœ¬ã€‚

æ”¯æ´æ–¹æ³•ï¼š
- LoRAï¼ˆæœ€å¸¸ç”¨ï¼‰
- Prompt Tuning
- Prefix Tuning
- AdaLoRA

å®˜æ–¹ repo: https://github.com/huggingface/peft

---

### âœ… LoRAï¼ˆLow-Rank Adaptationï¼‰

> LoRA æ˜¯ä¸€ç¨®å°‡åŸå§‹æ¨¡å‹åƒæ•¸å‡çµï¼Œåªåœ¨æŸäº›å±¤æ’å…¥ä½ç§©çŸ©é™£é€²è¡Œè¨“ç·´çš„æ–¹æ³•ã€‚

- å¤§å¹…é™ä½ VRAM ä½¿ç”¨é‡
- å¯èˆ‡åŸå§‹æ¨¡å‹åˆä½µæˆ–åˆ†é›¢å„²å­˜
- å°æ–¼ LLaMAã€Falconã€Mistralã€GPT2 ç­‰æ¶æ§‹ç‰¹åˆ¥æœ‰æ•ˆ

---

## ğŸ—‚ï¸ å°ˆæ¡ˆç›®éŒ„æ¶æ§‹è¨­è¨ˆï¼ˆå¯¦æˆ°æ¨è–¦ï¼‰

```bash
llm-finetune/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ lora_config.json          # PEFT è¨­å®š
â”œâ”€â”€ data/
â”‚   â””â”€â”€ train.jsonl               # è¼¸å…¥è³‡æ–™é›†
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_finetune.py          # ä¸»è¨“ç·´è…³æœ¬
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ adapter_model/           # å„²å­˜å¾®èª¿å¾Œåƒæ•¸ï¼ˆå¦‚ LoRA adapterï¼‰
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ training.log
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§ª å¯¦ä½œæµç¨‹æ•™å­¸ï¼šä½¿ç”¨ Hugging Face + PEFT + LoRA

### 1ï¸âƒ£ å®‰è£å¿…è¦å¥—ä»¶

```bash
pip install transformers datasets peft accelerate
```

---

### 2ï¸âƒ£ å»ºç«‹è¨“ç·´è³‡æ–™ï¼ˆæ ¼å¼ç‚º jsonlï¼‰

```json
{"instruction": "ç¿»è­¯æˆè‹±æ–‡ï¼šæˆ‘æ„›ä½ ", "input": "", "output": "I love you"}
{"instruction": "è§£é‡‹ä»€éº¼æ˜¯ AIï¼Ÿ", "input": "", "output": "AI æ˜¯äººå·¥æ™ºæ…§çš„ç¸®å¯«..."}
```

---

### 3ï¸âƒ£ è¨“ç·´è…³æœ¬ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import get_peft_model, LoraConfig, TaskType
from datasets import load_dataset

model_name = "tiiuae/falcon-7b"  # å¯æ›¿æ›æˆ llama2ã€gpt2 ç­‰

model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1
)

model = get_peft_model(model, peft_config)

# è¼‰å…¥è³‡æ–™ï¼ˆéœ€è™•ç† tokenizationï¼‰
dataset = load_dataset("json", data_files="data/train.jsonl")["train"]

# è¨“ç·´åƒæ•¸
args = TrainingArguments(
    output_dir="checkpoints",
    per_device_train_batch_size=2,
    num_train_epochs=3,
    logging_dir="logs",
    save_strategy="epoch"
)

# é–‹å§‹è¨“ç·´
from transformers import Trainer
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=dataset
)
trainer.train()
```

---

## ğŸ“¤ å¦‚ä½•ä½¿ç”¨å¾®èª¿å¾Œçš„ LoRA æ¨¡å‹ï¼Ÿ

```python
from peft import PeftModel

base_model = AutoModelForCausalLM.from_pretrained("tiiuae/falcon-7b")
lora_model = PeftModel.from_pretrained(base_model, "checkpoints/adapter_model")

# æ¨è«–
output = lora_model.generate(...)
```

---

## ğŸ“¦ æ¨¡å‹ä¸Šå‚³è‡³ Hugging Face

```bash
from huggingface_hub import HfApi
api = HfApi()
api.upload_folder(folder_path="checkpoints/adapter_model", repo_id="your-username/your-model")
```

---

## âœ… å¯¦å‹™å»ºè­°èˆ‡æœ€ä½³å¯¦è¸

* ğŸ”’ ä½¿ç”¨ `bnb` æˆ– `8bit` æ¨¡å‹ç¯€çœè¨˜æ†¶é«”ï¼ˆbitsandbytesï¼‰
* ğŸš€ ä½¿ç”¨ `accelerate` é€²è¡Œå¤šå¡åˆ†æ•£è¨“ç·´
* ğŸ“‹ å°è³‡æ–™é›†å¯ç”¨ `gradient checkpointing` é™ä½é¡¯å­˜æ¶ˆè€—
* ğŸ§ª æ¸¬è©¦ prompt ä¸€è‡´æ€§ã€å›ç­”é•·åº¦èˆ‡æ ¼å¼æº–ç¢ºåº¦

---

## ğŸ“˜ å»¶ä¼¸è³‡æºæ¨è–¦

* [Hugging Face PEFT å®˜æ–¹æ–‡ä»¶](https://huggingface.co/docs/peft/index)
* [LoRA åŸå§‹è«–æ–‡](https://arxiv.org/abs/2106.09685)
* [LLaMAã€GPT2 ç­‰å¾®èª¿å¯¦ä¾‹](https://github.com/huggingface/transformers/tree/main/examples)

---

## âœ… çµèª

å¾®èª¿å¤§å‹èªè¨€æ¨¡å‹å·²ç¶“ä¸å†æ˜¯åªæœ‰å¤§å…¬å¸èƒ½åšçš„äº‹ã€‚é€é LoRA èˆ‡ PEFTï¼Œä½ å¯ä»¥ç”¨å–®å¼µ GPUã€ç”šè‡³ Colab å°±å¾®èª¿å‡ºé©åˆè‡ªå·±çš„ä»»å‹™æ¨¡å‹ã€‚å¸Œæœ›é€™ç¯‡æ–‡ç« èƒ½è®“ä½ è·¨å‡ºç¬¬ä¸€æ­¥ï¼Œåœ¨ LLM çš„ä¸–ç•Œä¸­æ‰“é€ å±¬æ–¼è‡ªå·±çš„æ™ºæ…§ç³»çµ±ã€‚

